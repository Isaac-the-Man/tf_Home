{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is training the breakfast model, but using dnn instead of linear regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS = ['num_sandwitch', 'num_salad','num_coke','total_cost']\n",
    "LABEL_COLUMN = 'total_cost'\n",
    "DEFAULTS = [[0], [0], [0], [0]]\n",
    "\n",
    "def read_dataset(filename, mode, batch_size = 512):\n",
    "    def _input_fn():\n",
    "        def decode_csv(value_column):\n",
    "            columns = tf.decode_csv(value_column, record_defaults= DEFAULTS)\n",
    "            features = dict(zip(CSV_COLUMNS, columns))\n",
    "            label = features.pop(LABEL_COLUMN)\n",
    "            return features, label if mode != tf.estimator.ModeKeys.PREDICT else features\n",
    "        \n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        filenames_dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read lines from text files\n",
    "        textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset)\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = textlines_dataset.map(decode_csv)\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:        # if training\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:        # if evaluating or validating, epochs = 1 since it doesn't have to repeat\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "        \n",
    "        # make the dataset repeat as the number of epochs, and then batch them to specific batch size\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "    \n",
    "        return dataset.make_one_shot_iterator().get_next()        # # Creates an Iterator for enumerating the elements of this dataset\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLUMNS = [\n",
    "    tf.feature_column.numeric_column('num_sandwitch'),\n",
    "    tf.feature_column.numeric_column('num_salad'),\n",
    "    tf.feature_column.numeric_column('num_coke'),\n",
    "]\n",
    "\n",
    "def add_more_features(feats):\n",
    "  # Nothing to add (yet!)\n",
    "  return feats\n",
    "\n",
    "feature_cols = add_more_features(INPUT_COLUMNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serving Input Function\n",
    "# Defines the expected shape of the JSON feed that the model\n",
    "# will receive once deployed behind a REST API in production.\n",
    "def serving_input_fn():\n",
    "  feature_placeholders = {\n",
    "    'num_sandwitch' : tf.placeholder(tf.int32, shape=[None]),\n",
    "    'num_salad' : tf.placeholder(tf.int32, shape=[None]),\n",
    "    'num_coke' : tf.placeholder(tf.int32, shape=[None]),\n",
    "  }\n",
    "  # You can transforma data here from the input format to the format expected by your model.\n",
    "  features = feature_placeholders # no transformation needed\n",
    "  return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)  # input_fn for training and for commercial use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(output_dir, num_train_steps):\n",
    "    estimator = tf.estimator.DNNRegressor(\n",
    "        model_dir=output_dir,\n",
    "        feature_columns=feature_cols,\n",
    "        hidden_units=[32,16,8],\n",
    "        optimizer=tf.train.AdamOptimizer(\n",
    "            learning_rate=0.1,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # train specs are parameters to run training\n",
    "    train_spec=tf.estimator.TrainSpec(\n",
    "                       input_fn = read_dataset('./data/breakfast_train*.csv', mode = tf.estimator.ModeKeys.TRAIN),\n",
    "                       max_steps = num_train_steps)        # use max_steps here since epochs is infinite\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "\n",
    "    # eval specs are parameters to run evaluation\n",
    "    eval_spec=tf.estimator.EvalSpec(\n",
    "                       input_fn = read_dataset('./data/breakfast_valid*.csv', mode = tf.estimator.ModeKeys.EVAL),\n",
    "                       steps = None,        # no stepping because evaluation is for testing accuracy, not training\n",
    "                       start_delay_secs = 1, # start evaluating after N seconds\n",
    "                       throttle_secs = 1,  # evaluate every N seconds\n",
    "                       exporters = exporter)        # eval is designed to be realistic, so exporter instead of training dataset will be used here\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = './output_breakfast2'\n",
    "#TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './output_breakfast2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f663c770c18>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "estimator = tf.estimator.DNNRegressor(\n",
    "    model_dir=OUTDIR,\n",
    "    feature_columns=feature_cols,\n",
    "    hidden_units=[32,16,8],\n",
    "    optimizer=tf.train.AdamOptimizer(\n",
    "        learning_rate=0.1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predict_input_fn(num_san, num_sal, num_cok):\n",
    "    def predict_input_fn():\n",
    "        \"\"\"An input function for prediction\"\"\"\n",
    "        # Convert the inputs to a Dataset.\n",
    "        dataset = tf.data.Dataset.from_tensor_slices({\n",
    "                'num_sandwitch' : [num_san],\n",
    "                'num_salad' : [num_sal],\n",
    "                'num_coke' : [num_cok]\n",
    "            })\n",
    "        dataset = dataset.batch(1)\n",
    "        # Return the dataset.\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "    return predict_input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sandwitch: 10\n",
      "Salad: 10\n"
     ]
    }
   ],
   "source": [
    "# predict according to input\n",
    "num_san = int(input('Sandwitch: '))\n",
    "num_sal = int(input('Salad: '))\n",
    "num_cok = int(input('Coke: '))\n",
    "\n",
    "predictions = estimator.predict(input_fn=make_predict_input_fn(num_san,num_sal,num_cok))\n",
    "print(next(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "# only run this when you want to re-train\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "estimator = train_and_evaluate(OUTDIR, num_train_steps = 80000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
